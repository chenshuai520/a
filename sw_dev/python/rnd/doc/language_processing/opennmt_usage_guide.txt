[-] General.
	- Site.
		http://opennmt.net/

		https://github.com/OpenNMT/OpenNMT-py
		https://github.com/OpenNMT/OpenNMT-tf
		https://github.com/OpenNMT/CTranslate2
		https://github.com/OpenNMT/Tokenizer

	- Documentation.
		http://opennmt.net/OpenNMT-py/
		http://opennmt.net/OpenNMT-tf/

[-] Usage (OpenNMT-py).
	- Preprocess.
		REF [site] >> https://opennmt.net/OpenNMT-py/options/preprocess.html

		onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo
		python preprecess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo
			This will yield :
				data/demo.train.pt: serialized PyTorch file containing training data.
				data/demo.valid.pt: serialized PyTorch file containing validation data.
				data/demo.vocab.pt: serialized PyTorch file containing vocabulary data.

	- Train.
		REF [site] >> https://opennmt.net/OpenNMT-py/options/train.html

		onmt_train -data data/demo -save_model demo-model
		python train.py -data data/demo -save_model demo-model
			This will run the default model, which consists of a 2-layer LSTM with 500 hidden units on both the encoder/decoder.
			If you want to train on GPU, you need to set, as an example:
				CUDA_VISIBLE_DEVICES=1,3 -world_size 2 -gpu_ranks 0 1 to use (say) GPU 1 and 3 on this node only.

	- Translate.
		REF [site] >> https://opennmt.net/OpenNMT-py/options/translate.html

		onmt_translate -model demo-model_XYZ.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose
		python translate.py -model demo-model_XYZ.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose

	- Server.
		REF [site] >> https://opennmt.net/OpenNMT-py/options/server.html

[-] Usage (OpenNMT-py, Translation).
	https://opennmt.net/OpenNMT-py/extended.html

[-] Usage (OpenNMT-py, Summarization).
	https://opennmt.net/OpenNMT-py/Summarization.html

[-] Usage (OpenNMT-py, Image to Text).
	https://opennmt.net/OpenNMT-py/im2text.html

	- Download.
		wget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz
		tar zxf data/im2text.tgz -C data/

	- Preprocess.
		onmt_preprocess -data_type img \
			-src_dir data/im2text/images/ \
			-train_src data/im2text/src-train.txt -train_tgt data/im2text/tgt-train.txt \
			-valid_src data/im2text/src-val.txt -valid_tgt data/im2text/tgt-val.txt \
			-save_data data/im2text/demo \
			-tgt_seq_length 150 \
			-tgt_words_min_frequency 2 \
			-shard_size 500 \
			-image_channel_size 3
			
		onmt_preprocess -data_type img \
			-src_dir data/im2text_small/images/ \
			-train_src data/im2text_small/src-train.txt -train_tgt data/im2text_small/tgt-train.txt \
			-valid_src data/im2text_small/src-val.txt -valid_tgt data/im2text_small/tgt-val.txt \
			-save_data data/im2text_small/demo \
			-tgt_seq_length 150 \
			-tgt_words_min_frequency 2 \
			-shard_size 500 \
			-image_channel_size 3

			Output:
				data/im2text_small/demo.train.0.pt
				data/im2text_small/demo.train.1.pt
				data/im2text_small/demo.valid.0.pt
				data/im2text_small/demo.vocab.pt

		src_dir: Source directory for image or audio files.
		train_src, valid_src: Path to the training/validation source data (relative to src_dir).
			<image0_path>
			<image1_path>
			<image2_path>
			...
		train_tgt, valid_tgt: Path to the training/validation target data.
			<label0_token0> <label0_token1> ... <label0_tokenN0>
			<label1_token0> <label1_token1> ... <label1_tokenN1>
			<label2_token0> <label2_token1> ... <label2_tokenN2>
			...

	- Train.
		CUDA_VISIBLE_DEVICES=0 onmt_train -model_type img \
			-data data/im2text/demo \
			-save_model demo-model \
			-gpu_ranks 0 \
			-world_size 1 \
			-batch_size 20 \
			-max_grad_norm 20 \
			-learning_rate 0.1 \
			-word_vec_size 80 \
			-encoder_type brnn \
			-image_channel_size 1

			Output:
				./demo-model_step_?????.pt

	- Translate.
		onmt_translate -data_type img -model model.pt \
			-src_dir /path/to/image_dir \
			-src /path/to/image_list_file \
			-output /path/to/output_file \
			-max_length 150 -beam_size 5 -gpu 0 -verbose

		src_dir: Source directory for image or audio files.
		src: Source sequence to decode (one line per sequence) (relative to src_dir).
		tgt: True target sequence (optional).

		onmt_translate -data_type img \
			-model demo-model_acc_x_ppl_x_e13.pt \
			-src_dir data/im2text/images \
			-src data/im2text/src-test.txt \
			-output pred.txt \
			-max_length 150 \
			-beam_size 5 \
			-gpu 0 \
			-verbose

[-] Usage (OpenNMT-py, Speech to Text).
	https://opennmt.net/OpenNMT-py/speech2text.html

[-] Usage (OpenNMT-py, Video to Text).
	https://opennmt.net/OpenNMT-py/vid2text.html

[-] Usage (OpenNMT-tf).
	onmt-main -h

	- Preprocess.
		Build the source and target word vocabularies from the training files:
			onmt-build-vocab --size 50000 --save_vocab src-vocab.txt src-train.txt
			onmt-build-vocab --size 50000 --save_vocab tgt-vocab.txt tgt-train.txt

		The data files should be declared in a YAML configuration file (let's name it my_config.yml):
			model_dir: run/

			data:
				train_features_file: src-train.txt
				train_labels_file: tgt-train.txt
				eval_features_file: src-val.txt
				eval_labels_file: tgt-val.txt
				source_vocabulary: src-vocab.txt
				target_vocabulary: tgt-vocab.txt

	- Train.
		onmt-main --model_type Transformer --config my_config.yml --auto_config train --with_eval
		python opennmt/bin/main.py --model_type Transformer --config my_config.yml --auto_config train --with_eval
			The --auto_config flag selects the best settings for this type of model.

			The training will regularly produce checkpoints in the run/ directory.
			To monitor the training progress, some logs are displayed in the console.
			However, to visually monitor the training we suggest using TensorBoard:
				tensorboard --logdir="run"

	- Translate.
		The average_checkpoints run type can be used to average the parameters of several checkpoints, usually increasing the model performance.
			onmt-main --config my_config.yml --auto_config average_checkpoints --output_dir run/baseline-enfr/avg --max_count 5
			python opennmt/bin/main.py --config my_config.yml --auto_config average_checkpoints --output_dir run/baseline-enfr/avg --max_count 5
				The average_checkpoints run type can be used to average the parameters of several checkpoints, usually increasing the model performance.

		Execute the inference by setting the --checkpoint_path option:
			onmt-main --config my_config.yml --auto_config --checkpoint_path run/baseline-enfr/avg/ckpt-315000 infer --features_file src-test.txt --predictions_file src-test.txt.out
			python opennmt/bin/main.py --config my_config.yml --auto_config --checkpoint_path run/baseline-enfr/avg/ckpt-315000 infer --features_file src-test.txt --predictions_file src-test.txt.out
				The most recent checkpoint will be used by default.
				The predictions will be printed on the standard output.

		Score existing translations via the score run type:
			onmt-main --config my_config.yml --auto_config score --features_file src-test.txt --predictions_file src-test.txt.out
			python opennmt/bin/main.py --config my_config.yml --auto_config score --features_file src-test.txt --predictions_file src-test.txt.out

	- Other run type.
		python opennmt/bin/main.py ... eval ...
		python opennmt/bin/main.py ... export ...
		python opennmt/bin/main.py ... update_vocab ...

[-] Installation.
	- Install using pip.
		pip install OpenNMT-py
		pip install OpenNMT-tf

	- Install from sources.
		cd ${OpenNMT-py_HOME}
		python setup.py install

		(Optional) some advanced features (e.g. working audio, image or pretrained models) requires extra packages:
			pip install -r requirements.opt.txt

[-] Troubleshooting.
	- <error> AttributeError: 'InputFeedRNNDecoder' object has no attribute '_input_size'
		<cause> ?
		<solution>
			Assign the parameter 'embeddings' in InputFeedRNNDecoder.__init__().
