#!/usr/bin/env python

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import os
import numpy as np
import tensorflow as tf
import edward as ed
from edward.models import Uniform
from observations import mnist
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

def plot(samples):
	fig = plt.figure(figsize=(4, 4))
	gs = gridspec.GridSpec(4, 4)
	gs.update(wspace=0.05, hspace=0.05)

	for i, sample in enumerate(samples):
		ax = plt.subplot(gs[i])
		plt.axis('off')
		ax.set_xticklabels([])
		ax.set_yticklabels([])
		ax.set_aspect('equal')
		plt.imshow(sample.reshape(28, 28), cmap='Greys_r')

	return fig

def generator(array, batch_size):
	"""Generate batch with respect to array's first axis."""
	start = 0  # Pointer to where we are in iteration.
	while True:
		stop = start + batch_size
		diff = stop - array.shape[0]
		if diff <= 0:
			batch = array[start:stop]
			start += batch_size
		else:
			batch = np.concatenate((array[start:], array[:diff]))
			start = diff
		batch = batch.astype(np.float32) / 255.0  # Normalize pixel intensities.
		batch = np.random.binomial(1, batch)  # Binarize images.
		yield batch

def generative_network(eps):
	net = tf.layers.dense(eps, 128, activation=tf.nn.relu)
	net = tf.layers.dense(net, 784, activation=tf.sigmoid)
	return net

def discriminative_network(x):
	"""Outputs probability in logits."""
	net = tf.layers.dense(x, 128, activation=tf.nn.relu)
	net = tf.layers.dense(net, 1, activation=None)
	return net

# REF [site] >> http://edwardlib.org/tutorials/gan
def generative_adversarial_network_example():
	ed.set_seed(42)

	data_dir = '/tmp/data'
	out_dir = '/tmp/out'
	if not os.path.exists(out_dir):
		os.makedirs(out_dir)
	M = 128  # Batch size during training.
	d = 100  # Latent dimension.

	(x_train, _), (x_test, _) = mnist(data_dir)
	x_train_generator = generator(x_train, M)
	x_ph = tf.placeholder(tf.float32, [M, 784])

	#--------------------
	# GANs posit generative models using an implicit mechanism.
	# Given some random noise, the data is assumed to be generated by a deterministic function of that noise.
	with tf.variable_scope('Gen'):
		eps = Uniform(tf.zeros([M, d]) - 1.0, tf.ones([M, d]))
		x = generative_network(eps)

	#--------------------
	# In Edward, the GAN algorithm (GANInference) simply takes the implicit density model on x as input, binded to its realizations x_ph.
	# In addition, a parameterized function discriminator is provided to distinguish their samples.
	inference = ed.GANInference(data={x: x_ph}, discriminator=discriminative_network)

	# We'll use ADAM as optimizers for both the generator and discriminator.
	# We'll run the algorithm for 15,000 iterations and print progress every 1,000 iterations.
	optimizer = tf.train.AdamOptimizer()
	optimizer_d = tf.train.AdamOptimizer()

	inference = ed.GANInference(data={x: x_ph}, discriminator=discriminative_network)
	inference.initialize(optimizer=optimizer, optimizer_d=optimizer_d, n_iter=15000, n_print=1000)

	# We now form the main loop which trains the GAN.
	# At each iteration, it takes a minibatch and updates the parameters according to the algorithm.
	sess = ed.get_session()
	tf.global_variables_initializer().run()

	idx = np.random.randint(M, size=16)
	i = 0
	for t in range(inference.n_iter):
		if t % inference.n_print == 0:
			samples = sess.run(x)
			samples = samples[idx, ]

			fig = plot(samples)
			plt.savefig(os.path.join(out_dir, '{}.png').format(str(i).zfill(3)), bbox_inches='tight')
			plt.close(fig)
			i += 1

		x_batch = next(x_train_generator)
		info_dict = inference.update(feed_dict={x_ph: x_batch})
		inference.print_progress(info_dict)

	#--------------------
	# Criticism
	# Evaluation of GANs remains an open problem---both in criticizing their fit to data and in assessing convergence.

def main():
	generative_adversarial_network_example()

#%%------------------------------------------------------------------

if '__main__' == __name__:
	main()
