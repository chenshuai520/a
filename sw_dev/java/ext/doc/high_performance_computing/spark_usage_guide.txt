[-] General.
	- Site.
		http://spark.apache.org/

	- Book.
		"Learning Spark", 2015.
			https://github.com/holdenk/learning-spark-examples
		"Learning PySpark", 2017.
			https://github.com/PacktPublishing/Learning-PySpark
		"High Performance Spark", 2017.

[-] UI.
	- SparkUI: Spark web UI.
		REF [book] >> "Learning Spark", p. 150.
		http://masternode:4040/

	- Cluster manager's web UI.
		REF [book] >> "Learning Spark", p. 130.
		http://masternode:8080/

[-] Cluster manager.
	- Standalone Cluster Manager.
		REF [book] >> "Learning Spark", p. 129.

		Fill in the workers' hostnames:
			$SPARK_HOME/conf/slave
		Start the cluster:
			sh $SPARK_HOME/sbin/start-all.sh
		Stop the cluster:
			sh $SPARK_HOME/sbin/stop-all.sh

		Open Spark shell:
			spark-shell --master spark://masternode:7077
			pyspark --master spark://masternode:7077
		Run a Spark application in a Spark Standalone cluster:
			spark-submit --master spark://masternode:7077 spark_python_script.py

[-] Usage.
	- Run a Python script.
		spark-submit spark_python_script.py
		spark-submit --master local[4] spark_python_script.py
		spark-submit --master local[4] --packages mysql:mysql-connector-java:8.0.12 spark_python_script.py
			Run in local mode with 4 cores.
		spark-submit --master spark://host:7077 --executor-memory 10g spark_python_script.py
			Run in a Spark Standalone cluster.

	- Run a Scala application.
		When using sbt:
			sbt clean package

			spark-submit --class "com.sangwook.Main" --master local[4] target/scala-2.11/simple-example_2.11-1.0.0.jar
				Run in local mode with 4 cores.

		When using Maven:
			mvn archetype:generate
				Generate pom.xml and directories.
			mvn dependency:copy-dependencies
				Copy all libraries and dependencies to the "target/dependency" folder.
			mvn clean
			mvn compile && mvn package
				Compile, run tests, and create jar.

			spark-submit --class "com.sangwook.Main" --master local[4] target/simple-example-1.0.0.jar
				Run in local mode with 4 cores.

	- Run a Java application.
		When using Maven:
			mvn archetype:generate
				Generate pom.xml and directories.
			mvn dependency:copy-dependencies
				Copy all libraries and dependencies to the "target/dependency" folder.
			mvn clean
			mvn compile && mvn package
				Compile, run tests, and create jar.

			spark-submit --class "com.sangwook.Main" --master local[4] target/simple-example-1.0.0.jar

[-] Trouble shooting (Windows).
	- Run a Spark application.
		<error>
			Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
		<solution>
			REF [site] >> https://stackoverflow.com/questions/35652665/java-io-ioexception-could-not-locate-executable-null-bin-winutils-exe-in-the-ha

			Download winutils.exe
				http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe
				https://github.com/steveloughran/winutils
			Set environment variable.
				If winutils.exe is in ${WINUTILS_HOME}/bin:
					set HADOOP_HOME=${WINUTILS_HOME}

		<error>
			'cmd' is not recognized as an internal or external command, operable program or batch file.
		<solution>
			set PATH=%SystemRoot%\system32;%PATH%
